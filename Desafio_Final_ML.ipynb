{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f076055",
   "metadata": {},
   "source": [
    "\n",
    "# üß† Desafio Final ‚Äì Bootcamp Engenheiro(a) de Machine Learning\n",
    "Este notebook apresenta a resolu√ß√£o completa do desafio final do m√≥dulo de **Engenharia de Machine Learning**, utilizando o dataset `cars.csv`.  \n",
    "O pipeline segue os **7 passos fundamentais do Engenheiro de Machine Learning**, contemplando desde a an√°lise explorat√≥ria at√© a modelagem supervisionada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd55a82",
   "metadata": {},
   "source": [
    "## ü•á Passo 1 ‚Äì Compreens√£o do problema e dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f54ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Leitura do dataset (ajuste o caminho conforme o seu ambiente)\n",
    "df = pd.read_csv('/content/drive/MyDrive/cars.csv')\n",
    "\n",
    "# Visualiza√ß√µes iniciais\n",
    "print(\"Formato do dataset:\", df.shape)\n",
    "print(\"\\nColunas dispon√≠veis:\\n\", df.columns.tolist())\n",
    "display(df.head())\n",
    "print(\"\\nInforma√ß√µes gerais:\")\n",
    "df.info()\n",
    "display(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4da6ca",
   "metadata": {},
   "source": [
    "## üß© Passo 2 ‚Äì Coleta e explora√ß√£o inicial do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verifica√ß√£o de valores ausentes e duplicados\n",
    "print(\"Valores ausentes por coluna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nLinhas duplicadas:\", df.duplicated().sum())\n",
    "\n",
    "# Valores √∫nicos por coluna\n",
    "print(\"\\nValores √∫nicos por coluna:\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()} valores √∫nicos\")\n",
    "\n",
    "# Identifica√ß√£o de tipos de dados\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"\\nVari√°veis num√©ricas:\", num_cols)\n",
    "print(\"Vari√°veis categ√≥ricas:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a344b34",
   "metadata": {},
   "source": [
    "## üßπ Passo 3 ‚Äì Pr√©-processamento (limpeza e normaliza√ß√£o com `pd.to_numeric`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2412ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convers√£o segura de colunas num√©ricas com errors='coerce'\n",
    "for col in ['cubicinches', 'weightlbs']:\n",
    "    df[col] = df[col].astype(str).str.replace(',', '').str.strip()\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Substituir valores nulos pela m√©dia\n",
    "df['cubicinches'].fillna(df['cubicinches'].mean(), inplace=True)\n",
    "df['weightlbs'].fillna(df['weightlbs'].mean(), inplace=True)\n",
    "\n",
    "# Criar coluna de efici√™ncia com base no mpg\n",
    "df['efficiency'] = pd.cut(df['mpg'], bins=[0, 20, 30, df['mpg'].max()], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Normaliza√ß√£o das vari√°veis num√©ricas\n",
    "scaler = StandardScaler()\n",
    "num_features = ['cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60']\n",
    "\n",
    "df_scaled = df.copy()\n",
    "df_scaled[num_features] = scaler.fit_transform(df[num_features])\n",
    "\n",
    "print(\"Tipos de dados ap√≥s convers√£o:\\n\", df_scaled.dtypes)\n",
    "display(df_scaled.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bef56d",
   "metadata": {},
   "source": [
    "## üîç Passo 4 ‚Äì An√°lise explorat√≥ria e correla√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10761c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Matriz de correla√ß√£o\n",
    "corr = df_scaled[['mpg', 'cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60']].corr()\n",
    "\n",
    "print(\"Correla√ß√£o com a vari√°vel mpg:\\n\")\n",
    "print(corr['mpg'].sort_values(ascending=False))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr, annot=True, cmap='RdBu', center=0)\n",
    "plt.title('Matriz de Correla√ß√£o ‚Äì Vari√°veis Num√©ricas')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984a010",
   "metadata": {},
   "source": [
    "## üß≠ Passo 5 ‚Äì Redu√ß√£o de dimensionalidade com PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = df_scaled[['cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60']]\n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "print(\"Vari√¢ncia explicada por componente:\\n\", explained_var)\n",
    "print(\"\\nVari√¢ncia acumulada:\", explained_var.cumsum())\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(explained_var)+1), explained_var.cumsum(), marker='o')\n",
    "plt.xlabel('N√∫mero de Componentes Principais')\n",
    "plt.ylabel('Vari√¢ncia Acumulada Explicada')\n",
    "plt.title('PCA ‚Äì Vari√¢ncia Explicada Acumulada')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7f190",
   "metadata": {},
   "source": [
    "## üßÆ Passo 6 ‚Äì Clusteriza√ß√£o com K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X_pca2 = X_pca[:, :2]\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_pca2)\n",
    "\n",
    "df_scaled['cluster'] = clusters\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca2[:, 0], X_pca2[:, 1], c=clusters, cmap='viridis', s=60)\n",
    "plt.title('Clusters de Ve√≠culos (K-Means + PCA)')\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Distribui√ß√£o de ve√≠culos por cluster:\")\n",
    "print(df_scaled['cluster'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae2523",
   "metadata": {},
   "source": [
    "## üß† Passo 7 ‚Äì Modelagem supervisionada e avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X = df_scaled[['cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60']]\n",
    "y = df_scaled['efficiency']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "\n",
    "print(\"=== √Årvore de Decis√£o ===\")\n",
    "print(\"Acur√°cia:\", accuracy_score(y_test, y_pred_tree))\n",
    "print(classification_report(y_test, y_pred_tree))\n",
    "\n",
    "print(\"\\n=== Regress√£o Log√≠stica ===\")\n",
    "print(\"Acur√°cia:\", accuracy_score(y_test, y_pred_log))\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "\n",
    "import seaborn as sns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_tree), annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('√Årvore de Decis√£o')\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_log), annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title('Regress√£o Log√≠stica')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
