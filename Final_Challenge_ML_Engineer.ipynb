{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f420b98",
   "metadata": {},
   "source": [
    "\n",
    "# üß† Final Challenge ‚Äì Machine Learning Engineer Bootcamp\n",
    "This notebook presents the complete solution for the **Final Challenge** of the *Machine Learning Engineer Bootcamp*, using the `cars.csv` dataset.  \n",
    "The pipeline follows the **7 key steps of a Machine Learning Engineer**, covering the entire process from exploratory data analysis to supervised modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb7d8e",
   "metadata": {},
   "source": [
    "## ü•á Step 1 ‚Äì Understanding the problem and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19971456",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset (adjust the path according to your environment)\n",
    "df = pd.read_csv('/content/drive/MyDrive/cars.csv')\n",
    "\n",
    "# Initial exploration\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nAvailable columns:\\n\", df.columns.tolist())\n",
    "display(df.head())\n",
    "print(\"\\nGeneral info:\")\n",
    "df.info()\n",
    "display(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce44bd",
   "metadata": {},
   "source": [
    "## üß© Step 2 ‚Äì Data collection and initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb18b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for missing and duplicate values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nDuplicate rows:\", df.duplicated().sum())\n",
    "\n",
    "# Unique values per column\n",
    "print(\"\\nUnique values per column:\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "# Identify numeric and categorical variables\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"\\nNumeric columns:\", num_cols)\n",
    "print(\"Categorical columns:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe35b7",
   "metadata": {},
   "source": [
    "## üßπ Step 3 ‚Äì Preprocessing (cleaning and normalization using `pd.to_numeric`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Safe numeric conversion with errors='coerce'\n",
    "for col in ['cubicinches', 'weightlbs']:\n",
    "    df[col] = df[col].astype(str).str.replace(',', '').str.strip()\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Replace missing values with mean\n",
    "df['cubicinches'].fillna(df['cubicinches'].mean(), inplace=True)\n",
    "df['weightlbs'].fillna(df['weightlbs'].mean(), inplace=True)\n",
    "\n",
    "# Create efficiency column based on mpg\n",
    "df['efficiency'] = pd.cut(df['mpg'], bins=[0, 20, 30, df['mpg'].max()], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "num_features = ['cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60']\n",
    "\n",
    "df_scaled = df.copy()\n",
    "df_scaled[num_features] = scaler.fit_transform(df[num_features])\n",
    "\n",
    "print(\"Data types after conversion:\\n\", df_scaled.dtypes)\n",
    "display(df_scaled.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b524db",
   "metadata": {},
   "source": [
    "## üîç Step 4 ‚Äì Exploratory analysis and correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d020e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Correlation matrix\n",
    "corr = df_scaled[['mpg', 'cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60']].corr()\n",
    "\n",
    "print(\"Correlation with mpg:\\n\")\n",
    "print(corr['mpg'].sort_values(ascending=False))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr, annot=True, cmap='RdBu', center=0)\n",
    "plt.title('Correlation Matrix ‚Äì Numerical Variables')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503e460",
   "metadata": {},
   "source": [
    "## üß≠ Step 5 ‚Äì Dimensionality reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc590c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = df_scaled[['cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60']]\n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "print(\"Explained variance per component:\\n\", explained_var)\n",
    "print(\"\\nCumulative explained variance:\", explained_var.cumsum())\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(explained_var)+1), explained_var.cumsum(), marker='o')\n",
    "plt.xlabel('Principal Component Number')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA ‚Äì Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec388da",
   "metadata": {},
   "source": [
    "## üßÆ Step 6 ‚Äì Clustering with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X_pca2 = X_pca[:, :2]\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_pca2)\n",
    "\n",
    "df_scaled['cluster'] = clusters\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca2[:, 0], X_pca2[:, 1], c=clusters, cmap='viridis', s=60)\n",
    "plt.title('Vehicle Clusters (K-Means + PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Cluster distribution:\")\n",
    "print(df_scaled['cluster'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967322e",
   "metadata": {},
   "source": [
    "## üß† Step 7 ‚Äì Supervised modeling and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10d4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X = df_scaled[['cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60']]\n",
    "y = df_scaled['efficiency']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "\n",
    "print(\"=== Decision Tree ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
    "print(classification_report(y_test, y_pred_tree))\n",
    "\n",
    "print(\"\\n=== Logistic Regression ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_tree), annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Decision Tree')\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_log), annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title('Logistic Regression')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
